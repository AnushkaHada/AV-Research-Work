import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import cv2
import time

# Hyperparameters
BATCH_SIZE = 16
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.1
EPSILON_DECAY = 100000  # Slower decay for more early exploration
REPLAY_SIZE = 50000
LR = 1e-4
NUM_EPISODES = 2000
TARGET_UPDATE_FREQ = 1000
MAX_STEPS_PER_EPISODE = 400
TRAIN_EVERY = 4
TIME_LIMIT_PER_EPISODE = 60
TAU = 0.005  # For soft target updates

def preprocess(obs):
    gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
    return resized.astype(np.uint8)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

env = gym.make("CarRacing-v2", continuous=True, render_mode="rgb_array")

# More granular discrete actions
ACTIONS = [
    [0.0, 1.0, 0.0],     # straight
    [-0.25, 1.0, 0.0],   # slight left
    [0.25, 1.0, 0.0],    # slight right
    [-0.5, 1.0, 0.0],    # medium left
    [0.5, 1.0, 0.0],     # medium right
    [0.0, 0.0, 0.8]      # brake
]
NUM_ACTIONS = len(ACTIONS)

class DQN(nn.Module):
    def __init__(self, obs_shape, num_actions):
        super().__init__()
        c, h, w = obs_shape
        self.net = nn.Sequential(
            nn.Conv2d(c, 32, 8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, num_actions)
        )

    def forward(self, x):
        return self.net(x)

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.array, zip(*batch))
        return state, action, reward, next_state, done

    def __len__(self):
        return len(self.buffer)

class FrameStack:
    def __init__(self, k):
        self.k = k
        self.frames = deque([], maxlen=k)

    def reset(self, obs):
        frame = preprocess(obs)
        for _ in range(self.k):
            self.frames.append(frame)
        return self._get_stack()

    def step(self, obs):
        frame = preprocess(obs)
        self.frames.append(frame)
        return self._get_stack()

    def _get_stack(self):
        return np.stack(self.frames, axis=0)

q_net = DQN((4, 84, 84), NUM_ACTIONS).to(device)
target_net = DQN((4, 84, 84), NUM_ACTIONS).to(device)
target_net.load_state_dict(q_net.state_dict())
target_net.eval()

optimizer = optim.Adam(q_net.parameters(), lr=LR)
replay_buffer = ReplayBuffer(REPLAY_SIZE)

# Warm-up buffer
print("Warming up replay buffer...")
frame_stack = FrameStack(k=4)
obs, _ = env.reset()
state_np = frame_stack.reset(obs)
state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0).to(device) / 255.0
while len(replay_buffer) < 1000:
    action_idx = random.randrange(NUM_ACTIONS)
    action = ACTIONS[action_idx]
    next_obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    next_state_np = frame_stack.step(next_obs)
    next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0).to(device) / 255.0
    replay_buffer.push(
        state.squeeze(0).cpu().numpy(),
        action_idx,
        reward,
        next_state.squeeze(0).cpu().numpy(),
        done
    )
    state = next_state if not done else torch.tensor(
        frame_stack.reset(env.reset()[0]), dtype=torch.float32).unsqueeze(0).to(device) / 255.0

epsilon = EPSILON_START
step_count = 0

episode_rewards = []
epsilons = []
losses = []

for episode in range(NUM_EPISODES):
    obs, _ = env.reset()
    state_np = frame_stack.reset(obs)
    state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0).to(device) / 255.0
    episode_reward = 0
    done = False
    step = 0
    episode_start_time = time.time()
    neg_reward_count = 0

    while not done and step < MAX_STEPS_PER_EPISODE:
        step += 1
        step_count += 1
        epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-1.0 * step_count / EPSILON_DECAY)

        if random.random() < epsilon:
            action_idx = random.randrange(NUM_ACTIONS)
        else:
            with torch.no_grad():
                q_values = q_net(state)
                action_idx = q_values.argmax().item()

        action = ACTIONS[action_idx]
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        # Reward shaping
        reward += action[1] * 0.3  # bonus for gas
        reward -= abs(action[0]) * 0.1  # penalty for turning

        next_state_np = frame_stack.step(next_obs)
        next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0).to(device) / 255.0

        replay_buffer.push(
            state.squeeze(0).cpu().numpy(),
            action_idx,
            reward,
            next_state.squeeze(0).cpu().numpy(),
            done
        )

        state = next_state
        episode_reward += reward

        if reward < 0:
            neg_reward_count += 1
        if neg_reward_count > 50:
            print("Too many negative rewards, breaking.")
            break

        if step_count % TRAIN_EVERY == 0 and len(replay_buffer) > BATCH_SIZE:
            s, a, r, s_next, d = replay_buffer.sample(BATCH_SIZE)

            s = torch.tensor(s, dtype=torch.float32).to(device)
            s_next = torch.tensor(s_next, dtype=torch.float32).to(device)
            a = torch.tensor(a, dtype=torch.int64).to(device)
            r = torch.tensor(r, dtype=torch.float32).to(device)
            d = torch.tensor(d, dtype=torch.float32).to(device)

            q_values = q_net(s).gather(1, a.unsqueeze(1)).squeeze()
            with torch.no_grad():
                best_actions = q_net(s_next).argmax(1).unsqueeze(1)
                next_q_values = target_net(s_next).gather(1, best_actions).squeeze()
                targets = r + GAMMA * next_q_values * (1 - d)

            loss = nn.SmoothL1Loss()(q_values, targets)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(q_net.parameters(), 1.0)
            optimizer.step()
            losses.append(loss.item())

            # Soft update
            for target_param, param in zip(target_net.parameters(), q_net.parameters()):
                target_param.data.copy_(TAU * param.data + (1.0 - TAU) * target_param.data)

        if step_count % TARGET_UPDATE_FREQ == 0:
            target_net.load_state_dict(q_net.state_dict())

        if time.time() - episode_start_time > TIME_LIMIT_PER_EPISODE:
            print("Episode time limit exceeded.")
            break

    episode_rewards.append(episode_reward)
    epsilons.append(epsilon)
    print(f"Episode {episode} - Reward: {episode_reward:.2f} - Epsilon: {epsilon:.3f}")

    if episode % 10 == 0 and episode > 0:
        torch.save(q_net.state_dict(), f"checkpoint_episode_{episode}.pt")
        avg_reward = np.mean(episode_rewards[-10:])
        avg_loss = np.mean(losses[-100:]) if losses else 0
        print(f"[Episode {episode}] Avg Reward (last 10): {avg_reward:.2f}")
        print(f"[Episode {episode}] Avg Loss (last 100): {avg_loss:.5f}")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    frame_stack.reset(obs)

env.close()
