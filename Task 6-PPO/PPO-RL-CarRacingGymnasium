# Assignment includes running a PPO algorithm from scratch using Gymnasium environment.
# Shall use documentation provided in the Gymnasium website for a PPO algorithm implementation example: https://docs.agilerl.com/en/latest/tutorials/gymnasium/agilerl_ppo_tutorial.html 

# shall run PPO on gymnasium envierment. 
import gymnasium as gym 
# Implement the core paper from scratch and apply it to Car Racing Gymnasium Env
import os

from tqdm import trange
import imageio
import numpy as np
import torch

from agilerl.algorithms.ppo import PPO
from agilerl.hpo.mutation import Mutations
from agilerl.hpo.tournament import TournamentSelection
from agilerl.training.train_on_policy import train_on_policy
from agilerl.utils.utils import (
    create_population,
    make_vect_envs,
    observation_space_channels_to_first
)

# Documentation says its easier to define ALL hyperparameters in one dictdictionary.  
# This is done so we can easily alter parameters in one location
# Expanded on paramters names for clarity. 
INIT_HP = {
    "POPULATION_SIZE": 4,
    "BATCH_SIZE": 128, 
    "LEARNING_RATE": 0.001,
    "LEARNING_FREQUENCY": 1024,
    "DISCOUNT_FACTOR_GAMMA": 0.99,
    "LAMBDA_GENERAL_ADVANTAGE_ESTIMATION": 0.95,
    "ACTION_STANDARD_DEVIATION": 0.6,
    "SURROGATE_CLIP_COEFFICIENT": 0.2,
    "ENTROPY_COEFFICIENT": 0.01,
    "VALUE_FUNCTION_COEFFICIENT": 0.5,
    "MAX_NORM_GRADIENT_CLIP": 0.5,
    "TARGET_KL": None,
    "NUM_UPDATE_EPOCHS": 4,
    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]
    "CHANNELS_LAST": True,  # Use with RGB states
    "TARGET_SCORE": 200.0,  # Target score that will beat the environment
    "MAX_STEPS": 150000,  # Maximum number of steps an agent takes in an environment
    "EVO_STEPS": 10000,  # Evolution frequency
    "EVAL_STEPS": None,  # Number of evaluation steps per episode
    "EVAL_LOOP": 3,  # Number of evaluation episodes
    "TOURN_SIZE": 2,  # Tournament size
    "ELITISM": True,  # Elitism in tournament selection"
}

# Mutation parameters specific to Agielerl. might not need
MUT_P = {
    # Mutation probabilities
    "NO_MUT": 0.4,  # No mutation
    "ARCH_MUT": 0.2,  # Architecture mutation
    "NEW_LAYER": 0.2,  # New layer mutation
    "PARAMS_MUT": 0.2,  # Network parameters mutation
    "ACT_MUT": 0.2,  # Activation layer mutation
    "RL_HP_MUT": 0.2,  # Learning HP mutation
    "MUT_SD": 0.1,  # Mutation strength
    "RAND_SEED": 42,  # Random seed
}
num_envs = 8 # wrote it two times, so var can be used in other places
env = make_vect_envs("CarRacing-v2", num_envs = num_envs)

#Creates a vectorised environment and initialises the population of agents from the corresponding observation and action spaces.
# Basically gets information from the environment we create. 
observation_space = env.single_observation_space
action_space = env.single_action_space
if INIT_HP["CHANNELS_LAST"]:
    # Documentation adjusts dimensions for PyTorch API (C,H,W), for envs with RGB image states.
    observation_space = observation_space_channels_to_first(observation_space)


# Now we create a population of Agents
# Basically agents have hyperparameters and we find the best combination
# Each Agent is trained seperatly(Don't influence each other) on new data/clean environment. 
# The ones with the best combination of hyperparameters get passed on to the next round of training. 


# Set up the device. Common code for most ml algorithms. 
# use the superior GPU over cpu if its avalible.
# Since I run this on nautlus, I think I can use GPU.
device = "cuda" if torch.cuda.is_available() else "cpu"

# Defining basic network
net_config = {"head_config": {"hidden_size": [64,64]}}

# RL hyperparameters configuration for mutation during training
hp_config = HyperparametersConfig(
    lr = RLParameter(min = 1e-4,max=1e-2), # Set a range for the learning parameter. Which will change during training. 
    batch_size = RLParameter(min = 8, max = 1024, dtype = int) # Range of Batch size(sample data size) is set. 
    # The dtype is just data type. Which is integer. 
) 
#Define population using the previous variables. 
# observation, action space is gotten from the environment I created. Do not need to manually enter
pop = create_population(
    algo = "PPO", # set the algorithm to be PPO
    observation_space = observation_space, # state deminsion
    action_space = action_space, # action dimension
    net_config = net_config,
    INIT_HP = INIT_HP, # the hyperparameters that I put into dictinoary at the top. 
    hp_config = hp_config, # RL hyperparameters configuration for mutation during training
    population_size = INIT_HP["POPULATION_SIZE"], # the hyperparamter pop size is set.
    num_envs = num_envs,
    device = device,
)
# Creating Mutations and Tournament objects 
# From the documentation it is stated that Tournament selection is used to select the agents from a population.
# which will make up the next generation of agents. 
#If elitism is used, the best agent is kept for next generation. 
#Then, for each tournament, k individuals are randomly chosen, and the agent with the best evaluation fitness is preserved. 
#This is repeated until the population for the next generation is full.
tournament = TournamentSelection(
    INIT_HP["TOURN_SIZE"],
    INIT_HP["ELITISM"],
    INIT_HP["POPULATION_SIZE"],
    INIT_HP["EVAL_LOOP"]
) 
# Mutation is used to explore hyperparameter space. Allowing diff combinations to be tested. best combo gets based onto next gen
# Mutation class is used to mutate agents with pre-set probabilities. 
# available mutations currently implemented are:
# No mutation
# Network architecture mutation - adding layers or nodes. Trained weights are reused and new weights are initialized randomly.
# Network parameters mutation - mutating weights with Gaussian noise.
# Network activation layer mutation - change of activation layer.
# RL algorithm mutation - mutation of learning hyperparameter, such as learning rate or batch size.
#Mutations.mutation() returns a mutated population.
# Tournament selection and mutation should be applied sequentially to fully evolve a population between evaluation and learning cycles.
mutations = Mutations(
    no_mutation=MUT_P["NO_MUT"],
    architecture=MUT_P["ARCH_MUT"],
    new_layer_prob=MUT_P["NEW_LAYER"],
    parameters=MUT_P["PARAMS_MUT"],
    activation=MUT_P["ACT_MUT"],
    rl_hp=MUT_P["RL_HP_MUT"],
    mutation_sd=MUT_P["MUT_SD"],
    rand_seed=MUT_P["RAND_SEED"],
    device=device,
)

# Create custom training loop for agent to solve Gymnasium environment. 

total_steps = 0 # tracks the number of steps taken by ALL agents. 
print("Training ...")
# MAX_STEPS is the max number an agent takes in the env.
pbar = trange(INIT_HP["MAX_STEPS"], unit = "step") # pbar is the progress bar
# Each agent tracks its OWN steps in agent.steps[-1]. Gets the LAST step
# uses NumPy(np) to check if ALL agents have reached max step
# agent in pop just means it loops through the agents listed in the population. 
# np.less performs the comparision. 
while np.less([agent.steps[-1] for agent in pop], INIT_HP["MAX_STEPS"].all()):
    pop_episode_scores = []
    # trains agent one at a time
    for agent in pop:
        # reset env and score tracking. 
        state, info = env.reset()
        scores = np.zeros(num_envs) # scores stores episode scores for each parallel environment(can change num of env).
        completed_episode_scores = [] # stores scores when episode is completed. 
        for _ in range(-(INIT_HP["EVO_STEPS"]// -agent.learn_step)):
            # making list to store information 
            states = []
            actions = []
            log_probs = []
            rewards = []
            dones = []
            values = []
            # creates 1D numpy array with num_envs elements, all initialized to 0. 
            # if episode has ended RL returns done. 
            done = np.zeros(num_envs)
            learn_steps = 0 
            # Computes how many iterations are needed to reach the total evo_steps. 
            #  agent.learn_step is the number of steps agent learns per iteration. 
            # The // is normal division. So 1000evolstep//600learnstep = 16. 
            # However, 16 * 600 is 9600. This is less that 1000. Thus we round up by doing -(-a // b)
            for idex_step in range (-(agent.learn_step // -num_envs)):
                # collects rollout data. 
                #Loops this until total steps collected across all num_envs reaches agent.learn_step.
            
                if INIT_HP["CHANNELS_LAST"]: # converts to another format for image. 
                    state = obs_channels_to_first(state) 
                    # obs_channels_to_first is a helper function that reorders dimensions of state. 
                # Agent gets action from policy, interacts with env and records it
                action, log_prob, _, value = agent.get_action(state)
                # clip to action space. Processing the actions BEFORE sending to environment(env.step(action). 
                if isinstance(agent.action_space, spaces.Box):
                    # checks if agent is using continouse action space.  
                    # spaces.Box is a continuous range of actions 
                    if agent.actor.squask_output:
                        # checks if actor is already squashing its actions to keep it in range(ex: using tanh())
                        clipped_action = agent.actor.scale_action(action)
                    else:
                        # if not clipped, manually do it to keep within bounds as shown with .low and .high. 
                        clipped_action = np.clip(action, agent.action_space.low, agent.action_space.high)
                else:
                    # if not continous, then there is not need to scale/clip it. Thus we can use action as is. 
                    clipped_action = action 
                # All of the if-else above ensures the action is compatible with environment. 

                # applies action to the environment as shown with env.step(action). It then recives the result. 
                # In Gymnasium, returns 5 things: next_state, reward, terminated, truncated, info
                next_state, reward, terminated, truncated, info = env.step(action)
                # Checks if episode is done. astype(np.int8) converts T and F to 0 and 1. Boolean to integer.
                # useful when storing information on episodes being done or not. 
                # terminated: episode ended naturally. Goal met or failure. 
                # truncated = episode was cut short(max steps reached)
                next_done = np.logical_or(terminated, truncated).astype(np.int8)
                
                # we increment by num_envs because we are doing multiple envs at the same time. 
                # So if this agent takes 1 step in 1 env, then we need to add that for ALL envs. 
                total_steps += num_envs # This is the stopping criteria for the while loop
                steps += num_envs # tracks steps in current rollout
                learn_steps += num_envs # tracks steps towards learning 


                # append info to list. Note this is the initial items.  
                states.append(state)
                actions.append(action)
                log_probs.append(log_prob)
                rewards.append(reward)
                dones.append(done)
                values.append(value)

                # update items. 
                state = next_state
                done = next_done
                scores += np.array(reward) # add the reward gotten from the action taken in env. 

                # lopping through each env to check if done
                # d is True if terminated normally 
                # t is true if truncated
                # idx is the index of the env(ex: env 0, env 1)
                # for environment, d and t in terminated and trucnated. 
                for idx, (d, t) in enumerate(zip(terminated, truncated)):
                    # if episode is over
                    if d or t:
                        # scores[idx] is the scores/reward for that environment. 
                        # Append it to the completed episode score list. 
                        completed_episode_scores.append(scores[idx])
                        # agent scores is for long term information. 
                        agent.scores.append(scores[idx])
                        scores[idx] = 0 # reset score. 
            # progress update bar that updates based on steps taken by agent. 
            pbar.update(learn_steps // len(pop))
            
            if INIT_HP["CHANNELS_LAST"]:
                # checks if correct format. If channels appea last in the format, we change it for pytorch, so channel is first. 
                next_state = obs_channels_to_first(next_state)

            experiences = (
                states,
                actions,
                log_probs,
                rewards,
                dones,
                values,
                next_state,
                next_done,
            ) 
            # Learn according to agent's RL algorithm. Policy optimization happens
            agent.learn(experiences)

        agent.steps[-1] += steps # updates step counter for agent. good for plotting

        pop_episode_scores.append(completed_episode_scores) # stores scores (total reward) from completed episodes during THIS agents rollout. 

        # evaluate population. Gets fitness value for each agent. 
        fitnesses = [
            agent.test(
                env, 
                swap_channels = INIT_HP["CHANNELS_LAST"],
                max_steps = INIT_HP["EVAL_STEPS"],
                loop=INIT_HP["EVAL_LOOP"],
            )
            for agent in pop 
        ]

        # Takes each agents episode score and computs the mean. Only if they completed the episode. 
        mean_scores = [
            (
                np.mean(episode_scores)
                if len(episode_scores) > 0 
                else "0 completed episodes"
            )
            for episode_scores in pop_episode_scores
        ]


    print(f"--- Global steps {total_steps} ---")
    print(f"Steps {[agent.steps[-1] for agent in pop]}")
    print(f"Scores: {mean_scores}")
    print(f'Fitnesses: {["%.2f"%fitness for fitness in fitnesses]}')
    print(
        f'5 fitness avgs: {["%.2f"%np.mean(agent.fitness[-5:]) for agent in pop]}'
    )

    # Tournament selection and population mutation. 
    elite, pop = tournament.select(pop)
    pop = mutations.mutation(pop)

    # update step counter
    for agent in pop: 
        agent.steps.append(agent.steps[-1])

elite.save_checkpoint(save_path)
pbar.close()
env.close()
