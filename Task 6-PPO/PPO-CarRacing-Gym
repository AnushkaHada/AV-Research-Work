import torch
import torch.nn as nn
import numpy as np
import gymnasium as gym
from torch.distributions import Categorical

env = gym.make("CarRacing-v2", continuous=True)

import torch
import torch.nn as nn

class ActorCritic(nn.Module):
    def __init__(self, obs_shape, action_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Flatten(),
            nn.Linear(np.prod(obs_shape), 256),
            nn.ReLU(),
        )
        self.actor = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()  # because CarRacing has continuous action [-1,1]
        )
        self.critic = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, obs):
        base = self.shared(obs)
        return self.actor(base), self.critic(base)

    def act(self, obs):
        with torch.no_grad():
            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
            action_logits, value = self.forward(obs)
            action = action_logits.squeeze().numpy()
            return action, value.item()
class PPOAgent:
    def __init__(self, env, policy, optimizer, gamma=0.99, lam=0.95, eps_clip=0.2):
        self.env = env
        self.policy = policy
        self.optimizer = optimizer
        self.gamma = gamma
        self.lam = lam
        self.eps_clip = eps_clip
        self.training_logs = []

    def get_action(self, obs):
        action, value = self.policy.act(obs)
        return action, value

    def compute_returns(self, rewards, values, dones):
        # GAE or discounted returns
        returns = []
        gae = 0
        next_value = 0
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * next_value * (1 - dones[step]) - values[step]
            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae
            next_value = values[step]
            returns.insert(0, gae + values[step])
        return returns
obs_shape = env.observation_space.shape
action_dim = env.action_space.shape[0]

policy = ActorCritic(obs_shape, action_dim)
optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)
agent = PPOAgent(env, policy, optimizer)

n_episodes = 5

for episode in range(n_episodes):
    obs, _ = env.reset()
    done = False
    rewards, values, dones, observations, actions = [], [], [], [], []

    while not done:
        action, value = agent.get_action(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        observations.append(obs)
        actions.append(action)
        rewards.append(reward)
        values.append(value)
        dones.append(done)

        obs = next_obs

    # Compute returns and update
    returns = agent.compute_returns(rewards, values, dones)
# Convert to tensors
obs_tensor = torch.tensor(np.array(observations), dtype=torch.float32)
action_tensor = torch.tensor(np.array(actions), dtype=torch.float32)
return_tensor = torch.tensor(np.array(returns), dtype=torch.float32)
value_tensor = torch.tensor(np.array(values), dtype=torch.float32)

# Compute advantage
advantage = return_tensor - value_tensor

# Recompute actions and values for the update (important for PPO!)
action_pred, value_pred = policy(obs_tensor)
value_pred = value_pred.squeeze()

# Assume Gaussian policy for continuous actions
dist = torch.distributions.Normal(action_pred, 0.1)  # std could be learned too
log_probs = dist.log_prob(action_tensor).sum(axis=1)

# Compute new log_probs and old_log_probs
with torch.no_grad():
    old_log_probs = log_probs.clone()

# PPO ratio
ratio = torch.exp(log_probs - old_log_probs)

# PPO clipped surrogate objective
clip_adv = torch.clamp(ratio, 1 - agent.eps_clip, 1 + agent.eps_clip) * advantage
loss_actor = -torch.min(ratio * advantage, clip_adv).mean()

# Critic (value) loss
loss_critic = (return_tensor - value_pred).pow(2).mean()

# Total loss
loss = loss_actor + 0.5 * loss_critic

# Backpropagation
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f"Episode {episode}: Total reward = {sum(rewards):.2f}")
