import torch
import torch.nn as nn
import numpy as np
import gymnasium as gym
from torch.distributions import Categorical

env = gym.make("CarRacing-v2", continuous=True)

import torch
import torch.nn as nn

class ActorCritic(nn.Module):
    def __init__(self, obs_shape, action_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Flatten(),
            nn.Linear(np.prod(obs_shape), 256),
            nn.ReLU(),
        )
        self.actor_mean = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()  # action in [-1,1]
        )
        # Instead of outputting std as network output, define log_std as a learnable parameter:
        self.log_std = nn.Parameter(torch.zeros(action_dim))

        self.critic = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, obs):
        base = self.shared(obs)
        mean = self.actor_mean(base)
        # Expand log_std to match batch size:
        std = self.log_std.exp().expand_as(mean)
        return mean, std, self.critic(base)

    def act(self, obs):
        with torch.no_grad():
            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
            mean, std, value = self.forward(obs)
            dist = torch.distributions.Normal(mean, std)
            action = dist.sample()
            action = torch.clamp(action, -1, 1)
            log_prob = dist.log_prob(action).sum(dim=-1)
            return action.squeeze().numpy(), value.item(), log_prob.item()


class PPOAgent:
    def __init__(self, env, policy, optimizer, gamma=0.99, lam=0.95, eps_clip=0.2):
        self.env = env
        self.policy = policy
        self.optimizer = optimizer
        self.gamma = gamma
        self.lam = lam
        self.eps_clip = eps_clip
        self.training_logs = []

    def get_action(self, obs):
        action, value, log_prob = self.policy.act(obs)
        return action, value, log_prob

    def compute_returns(self, rewards, values, dones):
        # GAE or discounted returns
        returns = []
        gae = 0
        next_value = 0
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * next_value * (1 - dones[step]) - values[step]
            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae
            next_value = values[step]
            returns.insert(0, gae + values[step])
        return returns
obs_shape = env.observation_space.shape
action_dim = env.action_space.shape[0]

policy = ActorCritic(obs_shape, action_dim)
optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)
agent = PPOAgent(env, policy, optimizer)

n_episodes = 100

all_episode_rewards = []  # <--- Add this before the for-loop

for episode in range(n_episodes):
    obs, _ = env.reset()
    done = False
    rewards, values, dones, observations, actions = [], [], [], [], []

    old_log_probs = []
    while not done:
        action, value, log_prob = agent.get_action(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        observations.append(obs)
        actions.append(action)
        rewards.append(reward)
        values.append(value)
        dones.append(done)
        old_log_probs.append(log_prob)

        obs = next_obs

    # Compute returns and update
    returns = agent.compute_returns(rewards, values, dones)
    # Convert to tensors
    obs_tensor = torch.tensor(np.array(observations), dtype=torch.float32)
    action_tensor = torch.tensor(np.array(actions), dtype=torch.float32)
    return_tensor = torch.tensor(np.array(returns), dtype=torch.float32)
    value_tensor = torch.tensor(np.array(values), dtype=torch.float32)
    old_log_probs_tensor = torch.tensor(old_log_probs, dtype=torch.float32)

    # Compute advantage
    advantage = return_tensor - value_tensor

    # Recompute actions and values for the update (important for PPO!)
    mean_pred, std_pred, value_pred = policy(obs_tensor)
    value_pred = value_pred.squeeze()

    dist = torch.distributions.Normal(mean_pred, std_pred)
    log_probs = dist.log_prob(action_tensor).sum(dim=1)

    ratio = torch.exp(log_probs - old_log_probs_tensor) # PPO ratio

    # PPO clipped surrogate objective
    clip_adv = torch.clamp(ratio, 1 - agent.eps_clip, 1 + agent.eps_clip) * advantage
    loss_actor = -torch.min(ratio * advantage, clip_adv).mean()

    # Critic (value) loss
    loss_critic = (return_tensor - value_pred).pow(2).mean()

    # Total loss
    loss = loss_actor + 0.5 * loss_critic

    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    total_reward = sum(rewards)
    all_episode_rewards.append(total_reward)  # <--- Append rewards

    if episode % 5 == 0:
        print(f"Episode {episode}: Total reward = {total_reward:.2f}")
        print(f"Mean reward so far: {np.mean(all_episode_rewards[-100:]):.2f}")
