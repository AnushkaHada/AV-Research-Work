import torch
import torch.nn as nn

import numpy as np
import gymnasium as gym
from torch.utils.tensorboard import SummaryWriter



# Use GPU is avalible. And check if it actually uses it. 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# flatten and normalize the observations. This helped make the reward negative. 
def preprocess(obs):
    return obs.transpose(2, 0, 1).astype(np.float32).reshape(-1) / 255.0  

# As per Task 6, the environment is "CarRacing"
env = gym.make("CarRacing-v2", continuous=True)
# This is for tensorboard which I could not use due to Natulus being unavalible
writer = SummaryWriter(log_dir="runs/ppo_carracing")

# PPO needs an actor critic. Basically a model for policy(actor) and value estimation(critic)
class ActorCritic(nn.Module):
    def __init__(self, obs_shape, action_dim):
        super().__init__()
        # We have shared layers
        self.shared = nn.Sequential(
            nn.Flatten(),
            nn.Linear(np.prod(obs_shape), 256),
            nn.ReLU(),
        )
        # This is the mean of each action
        self.actor_mean = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()
        )
        # Uses Tahn just like how it was mentioned in the paper. This bounds it to 1 and -1 
        # Log_std is a learnable parameter for the standard deviation of the action distribution
        self.log_std = nn.Parameter(torch.ones(action_dim) * -0.5)
        self.critic = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    # basic CNN code
    def forward(self, obs):
        base = self.shared(obs)
        mean = self.actor_mean(base)
        std = self.log_std.exp().expand_as(mean)
        value = self.critic(base)
        return mean, std, value

    def act(self, obs):
        # Samples action from distribution and returns value, action, and log probability
        with torch.no_grad():
            obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)
            mean, std, value = self.forward(obs)
            dist = torch.distributions.Normal(mean, std)
            action = dist.sample()
            action = torch.clamp(action, -0.7, 0.7)
            log_prob = dist.log_prob(action).sum(dim=-1)
            return action.squeeze().cpu().numpy(), value.item(), log_prob.item()


class PPOAgent:
    # sets parameters
    def __init__(self, env, policy, optimizer, gamma=0.99, lam=0.95, eps_clip=0.2):
        self.env = env
        self.policy = policy
        self.optimizer = optimizer
        self.gamma = gamma
        self.lam = lam
        self.eps_clip = eps_clip

    # Generalized Advantage Estimation (GAE) as shown in research paper. 
    def compute_returns(self, rewards, values, dones):
        returns = []
        gae = 0
        next_value = 0
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * next_value * (1 - dones[step]) - values[step]
            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae
            next_value = values[step]
            returns.insert(0, gae + values[step])
        return returns

    def update(self, batch, epochs=4, batch_size=64):
        # changes data to tensors
        obs_tensor = torch.tensor(np.array(batch['obs']), dtype=torch.float32, device=device)
        action_tensor = torch.tensor(np.array(batch['actions']), dtype=torch.float32, device=device)
        return_tensor = torch.tensor(np.array(batch['returns']), dtype=torch.float32, device=device)
        old_log_probs_tensor = torch.tensor(batch['log_probs'], dtype=torch.float32, device=device)
        value_tensor = torch.tensor(batch['values'], dtype=torch.float32, device=device)

        # finds normalized advantage: return âˆ’ value
        advantage = return_tensor - value_tensor
        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)  # Normalize advantage

        dataset_size = len(batch['obs'])

        losses = {'actor': 0, 'critic': 0, 'total': 0}
        count = 0

        for _ in range(epochs):
            indices = np.arange(dataset_size)
            np.random.shuffle(indices)

            for start in range(0, dataset_size, batch_size):
                end = start + batch_size
                minibatch_idx = indices[start:end]

                mb_obs = obs_tensor[minibatch_idx]
                mb_actions = action_tensor[minibatch_idx]
                mb_returns = return_tensor[minibatch_idx]
                mb_old_log_probs = old_log_probs_tensor[minibatch_idx]
                mb_advantage = advantage[minibatch_idx]

                mean_pred, std_pred, value_pred = self.policy(mb_obs)
                value_pred = value_pred.squeeze()

                dist = torch.distributions.Normal(mean_pred, std_pred)
                log_probs = dist.log_prob(mb_actions).sum(dim=1)

                ratio = torch.exp(log_probs - mb_old_log_probs)
                clip_adv = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * mb_advantage
                loss_actor = -torch.min(ratio * mb_advantage, clip_adv).mean()
                loss_critic = (mb_returns - value_pred).pow(2).mean()
                loss = loss_actor + 0.5 * loss_critic

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                losses['actor'] += loss_actor.item()
                losses['critic'] += loss_critic.item()
                losses['total'] += loss.item()
                count += 1

        for key in losses:
            losses[key] /= count

        return losses


obs_shape = env.observation_space.shape
action_dim = env.action_space.shape[0]

policy = ActorCritic(obs_shape, action_dim).to(device)
optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)
agent = PPOAgent(env, policy, optimizer)

n_episodes = 1000
batch_size = 2048  # total steps before the update
mini_batch_size = 64
update_epochs = 10

all_episode_rewards = []

# collects all the information needed. The hyperparameters. 
batch = {
    'obs': [],
    'actions': [],
    'rewards': [],
    'values': [],
    'dones': [],
    'log_probs': [],
}

steps_collected = 0

obs, _ = env.reset()
obs = preprocess(obs)
done = False

for episode in range(n_episodes):
    episode_reward = 0
    while True:
        action, value, log_prob = agent.policy.act(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        next_obs = preprocess(next_obs)

        # Applies reward shaping
        reward += action[1] * 0.3 
        done = terminated or truncated

        batch['obs'].append(obs)
        batch['actions'].append(action)
        batch['rewards'].append(reward)
        batch['values'].append(value)
        batch['dones'].append(done)
        batch['log_probs'].append(log_prob)

        episode_reward += reward
        steps_collected += 1
        obs = next_obs

        if done:
            obs, _ = env.reset()
            obs = preprocess(obs)
            all_episode_rewards.append(episode_reward)
            print(f"Episode {episode} reward: {episode_reward:.2f}")
            writer.add_scalar("Reward/Episode", episode_reward, episode)
            if episode >= 9:
                mean_10 = np.mean(all_episode_rewards[-10:])
                writer.add_scalar("Reward/Mean_Last_10", mean_10, episode)
            episode_reward = 0
            break

        if steps_collected >= batch_size:
            returns = agent.compute_returns(batch['rewards'], batch['values'], batch['dones'])
            batch['returns'] = returns

            losses = agent.update(batch, epochs=update_epochs, batch_size=mini_batch_size)

            writer.add_scalar("Loss/Actor", losses['actor'], episode)
            writer.add_scalar("Loss/Critic", losses['critic'], episode)
            writer.add_scalar("Loss/Total", losses['total'], episode)

            batch = {k: [] for k in batch}
            steps_collected = 0


    # Just prints the mean average every 10 episodes. 
    if episode % 10 == 0:
        print(f"Episode {episode}: Mean reward last 10 episodes = {np.mean(all_episode_rewards[-10:]):.2f}")

writer.close()
